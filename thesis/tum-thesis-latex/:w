% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.
\chapter{Background}\label{chapter:Background}

\section{Integration of GPUs in Autonomous Driving Systems}

Autonomous driving system require \acsp{GPU} for the computational acceleration they provide to the many parallel machine learning models that interpret sensor data, make decisions and ensure safe navigation in real time. 
From perception to planning and control, each stage of the autonomous driving pipeline relies on models that must operate within tight latency constraints. 
These models include convolutional neural networks (CNNs) for image and object recognition, recurrent or transformer-based architectures for temporal sensor fusion and prediction, and reinforcement learning agents or decision trees for behavioral planning \cite{Krizhevsky_undated-ao}.
The sheer diversity and complexity of these tasks require a hardware platform that can execute thousands of operations in parallel in order to achieve the throughput necessary \cite{self_driving_learning}.

GPUs are particularily well suited to these workloads because of their massively parallel architecture and high memory bandwidth, which perfectly meet the demands of deep learning tasks. 
Unlike CPUs, which optimize for sequential instruction execution and low latency branching, GPUs are designed to handle large batches of matrix and tensor operations simultaneously. 
This makes them ideal for real time inference of deep neural networks.
Furthermore, modern GPU architectures provide specialized cores, such as tensor cores in NVIDIA GPUs, that are explicitly optimized for mixed precision matrix multiplication, which is a core operation in most machine learning models. 
By offloading intensive compute tasks to the GPU, autonomous systems can maintain low latency and high accuracy, both of which are crucial for safety and performance in dynamic driving environments.


\section{GPU versus CPU Architecture}
%\cite{taskparallelism} for cuda api not supporting interruption
\acsp{GPU} are capable of delivering this vast increase in throughput over CPUs as measured by GFLOPS, despite the lower clock rate, by simplifying the thread context in order to afford greater parallelism.
They were originally developed to accelerate graphics rendering, a task heavy in parallizable computations, which require only a very simple control overhead and as such have adapted the architecture to support as many possible different threads. 
Typical workloads designed for \acs{CPU} are based on sequential workloads, such as human input or complex logic, which requires complex thread overhead to speed up branches and I/O, through prefetching, branch prediction, and out of order execution.  
These \acsp{CPU} achieve higher single threaded performance by dedicating a "significant portion of transistors to non computational tasks like branch prediction and caching", which \acsp{GPU} can forgo in favor of increasing arithmetic intensity \cite{Owens2007-kp}.
Consider the following graphic Figure~\ref{fig:thread_complexity}, which highlights the difference in thread complexity. 


%This choice in architectural design makes \acsp{GPU} excell on data processing and machine learning workloads due to thier

%Both data processing and machine learning tasks rely extensively on intensive matrix computations, which can be easily parallelized.
%These tasks benefit, both in processing speed and model complexity, from the \acs{GPU}'s ability to perform thousands of operations in parallel, far greater than the traditional \acs{CPU} can achieve.

%\section{\acs{GPU} Limitations in Real Time Systems}

%\subsection{GPU vs CPU Threads}

%\acsp{GPU} have parallelised the thread centric scheduling execution model on \acsp{CPU} to reflect the architectural design.  
%\acsp{CPU} schedule threads to execute

%The fundamental differences betweeReal time systems \acs{GPU} threads differ significantly from \acs{CPU} threads, esulting
%To maximize computational and energy efficiency at scale, \acsp{GPU} maintain a minimal thread context in comparison to \acsp{CPU}. 
%\acsp{CPU} are engineered for general-purpose computing, where performance often involves improving single threaded execution.
%Achieving higher single-threaded performance means dedicating a "significant portion of transistors to non-computational tasks like branch prediction and caching" \cite{Owens2007-kp}.
%Instead, \acsp{GPU} sacrifice this complex control overhead to save transistors, which can be used for increasing the arithmetic intensity capability.
%This architectural choice is illustrated in Figure\Figure~\ref{fig:thread_complexity}, which highlights how the \acs{GPU}â€™s simplified control logic reduces overhead, allowing more transistors to be used for arithmetic units \cite{Owens2007-kp}.

\begin{figure}[htbp]
  \centering
  \resizebox{1.0\linewidth}{!}{
    \input{figures/cpuvgpu}
  }
  \caption{CPU vs GPU Thread Architecture}
  \label{fig:thread_complexity}
\end{figure}


Although core components are named differently, both CPU and GPU threads work fundamentally similarily with an instruction decoder, registers and an arithmetic unit. 
The differences arise when trying to maximize a single control flow. 
The CPU will prefetch instructions, reorder them to most effectively use the functional units and speculatively compute instructions based on a branch predictor. 
On the other hand, \acs{GPU} threads can not execute instructions out-of-order, use only manual prefetching, and have a simple branch predictor that is far more conservative than the \acs{CPU}'s predictor.
The additional complex logic involved allows single threaded \acs{CPU} applications to far outperform single threaded GPU applications, as seen by the following comparison of single threaded matrix multiplication in Figure~\ref{fig:singlethreadedgraph} and Figure~\ref{fig:singlethreadedmatrix}.

\begin{figure}[H]
  \centering 
  \resizebox{1.0\linewidth}{!}{
	  \input{figures/singlethreadedcomp.tex}
  }
  \caption{Single threaded Matrix Multiplication Execution between CPUs and GPUs averaged over 10 executions}
  \label{fig:singlethreadedgraph}
\end{figure}

\begin{figure}[H]
	\centering
	\resizebox{1.0\linewidth}{!}{
		\input{figures/comparematrix.tex}
	}
	\caption{Data Matrix from Figure~\ref{fig:singlethreadedgraph}}
	\label{fig:singlethreadedmatrix}
\end{figure}	




As seen in Figure~\ref{fig:singlethreadedgraph} and Figure~\ref{fig:singlethreadedmatrix} applications that seek to program \acsp{GPU} in a similar \acs{CPU} style will struggle to achieve high performance. 
For each of the matrice tested, through prefetching, a higher clock rate, prefetching 


In order to gain any performance in \acs{GPU} programming over pure \acs{CPU} programming, the \acs{GPU} architecture must be understood to 


\acsp{GPU} have revolutionized data processing and machine learning training and inference with their ability to handle massive amounts of data and execute simple, but highly-parallelized computations.
Data processing systems, based on traditional CPUs, rely on a \ac{MIMD} architecture that excels at handling complex control logic at high frequencies by executing different instructions on seperate data streams concurrently. 
However, for  tasks that require the repeated execution of singular instructions such as large-scale data processing workloads, \acsp{CPU} are burdened by the overhead of the complex control logic, where a simpler, more parallel design would be more effective.
\acs{GPU}s, created to overcome this limitation, use a \ac{SIMT} architecture with thousands of simpler, but slower threads executing simultaneously. 
% TODO
For example, the current architecture generally supports as many as 2048 threads in a single block, which can run on a single \ac{SM}, scaling the number of concurrent threads by the number of \acsp{SM}, by the number of \acsp{SM}, 80 on the newer models. 
% TODO
The parallelism provided by the \ac{SIMT} model allows a far higher throughput that outperforms \acsp{CPU}. 
Here, each data element is processed by its own thread, allowing the same instruction to be executed simultaneously across numerous data streams.

% TODO
Real time systems, which rely extensively on machine learning tasks and data processing tasks, Higher data processing performance is vital to autonomous systems, which rely extensively on machine learning tasks and sensor data processing. 
In autonomous systems a multitude of modules require deep learning, a machine learning technique, to automatically extract patterns, such as computer vision, and make decisions \cite{JEON2021167}.
Deep learning, inspired by the structure of the human brain, is composed of layers of numerous interconnected, identical nodes called neural networks.  
Neural network models rely on mathematical operations between different neighboring layers to perform inference, essentially the prediction making or recognition process.
The layers are represented as matrices, which then get multiplied and convoluted with one another and are used by specialized functions, called activation functions, to introduce non-linearity. 
With very large neural networks with thousands or millions of nodes, the inference computation is repetitive and slow. 
The capability to execute these repeptitive workloads concurrently across different dataset makes \acsp{GPU} fundamental to performance in tasks using complex neural networks.  
Furthermore, \acsp{GPU} are necessary to process the data rate produced by high-bandwidth, high-frequency sensors used in autonomous driving. 
The parallelism in \acs{GPU}s makes them the ideal platform over CPUs for deep learning and data processing tasks, tasks fundamental for autonomous driving systems.


Real time systems, such as autonomous driving, are designed with strict timing constraints in mind, to ensure predictable and deterministic behavior \cite{10155700}. 
deadlines, which are subdivided into soft and hard-deadlines. 
Hard deadlines are critical and a failure leads to the systems failure or unsafe conditions. 
For example, in autonomous driving, collision avoidance with another vehicle and brake activation are hard deadlines. 
If these deadlines are not met, the safety of the passengers and the system is at risk. 
On the other hand, soft deadlines are not critical and missing these deadlines degrades performance, but does not cause system failure. 
In autonomous driving, this would show in route planning and navigation updates, where a delay would lead to suboptimal paths, but safety is not comprimised. 
Real time systems need to be capable of effectively and efficiently switching from lower priority tasks, soft deadline tasks, to high priority, hard deadline tasks, to ensure the safety both for the passengers and nearby individuals.  





\subsection{Necessesity of \acsp{GPU} in Autonomous Driving}

\subsection{Autonomous Driving as a Real Time System}

\section{Background}
\subsection{Asynchronous Programming and Coroutines}

Asynchronous programming is a method of programming a system to handle tasks concurrently instead of sequentially. 
Typically used in conjunction with tasks that delay or have high wait-times, such as I/O heavy jobs, asynchronous programming reduces overall execution time by more efficiently using processing ressources. 
For example, while waiting for I/O heavy input like sensor I/O, asynchronous code lets other tasks execute in the meantime, before returning when the data arrives.  
For real-time systems, asynchronous programming additionally uses the intermittant execution model to enforce determinism. 
By allowing the \acsp{GPU} to switch between concurrent tasks, hard deadlines can be immediately enforced without delay. 


Coroutines, an implementation of asynchronous programming, uses suspendable functions to halt execution. 
Suspendable functions are implemented by capturing the current context, know as the continuation, of the currently running thread and save the data to be run later \cite{Zheng2022LuisaRender}. 
After being saved, a new process can take over execution, without interrupting or overwritting the state of the previous process. 
Once the intermittant process or higher priority process has finished execution, the original task can continue executing by restoring the process context, which was previously saved. 
Capturing the continuation of a function allows the resumption of the program to be strategically deferred. 

%Here, the scheduled tasks are independent of the control flow, meaning the execution order of the subtasks





\subsection{NVIDIA Tesla V100 Architecture}

For the purpose of this thesis, the NVIDIA Tesla V100 accelerator, which uses the Volta GV100 \ac{GPU} architecture, was selected due to its availability and high-performance computing capabilities.
At the core of its execution model is the warp, a group of 32 threads executing in \ac{SIMT} fashion \cite{nvidia2017tesla}.
Each warp is scheduled and dispatched within one of the 4 processing partitions of a \ac{SM}. 
The warp scheduler and warp dispatcher each handle 32 threads per clock cycle. 
Within each \ac{SM} partition, there are Tensor Cores for deep learning, 64 bit-\ac{FP} cores, \ac{LD/ST} units, a register file, and \acs{SFU}s for mathematical functions such as  sine and square root. 
Additionally, each partition contains 16 Cuda cores, which can execute both \ac{FP} 32 bit and \ac{INT} 32 bit instructions, but not simultaneously. 
Each partition has its own L0 instruction cache, while all partitions share a L1 instruction and data cache. 
A \ac{SM} can support up to 64 concurrent warps, allowing a maximum of $64 * 32 = 2048$ threads per \ac{SM}. 
At a higher level, two \acs{SM}s are grouped to form a \ac{TPC}. 
The Tesla V100 GPU consists of six \acs{GPC}s, each containing seven \acs{TPC}s, resulting in a total of 84 \acs{SM}s across the chip. 
This hierarchical organization, from warps to \acs{SM}s, \acs{TPC}s, and \acs{GPC}s, enables the Tesla V100 to efficiently handle massively parallel workloads, making it well-suited for high-performance computing and deel learning applications. 

 %\section{Motivation}
 %In autonomous driving and other real time system tasks, GPUs are essential for data processing and machine learning; however,  existing GPU scheduling techniques fail to meet the strict timing requirements of real-time applications.
 %GPU tasks are traditional queued and scheduled based on availabilty to optimize for high throughput applications like graphics rendering and offline machine learning training.
 %These tasks run to completion before switching tasks, meaning multiple tasks, such as the different modules in autonomous driving cannot efficiently share resources. 
 %Depending on the current workload for the GPU, execution times and latency can vary. 
 %These limitations make existing GPU scheduling mechanisms unsuitable for real-time applications like autonomous driving, where deadlines need to be met to ensure the safety of the system and passengers. 

%Citation test~\parencite{latex}.

%Acronyms must be added in \texttt{main.tex} and are referenced using macros. The first occurrence is automatically replaced with the long version of the acronym, while all subsequent usages use the abbreviation.

%E.g. \texttt{\textbackslash ac\{TUM\}, \textbackslash ac\{TUM\}} $\Rightarrow$ \ac{TUM}, \ac{TUM}

%For more details, see the documentation of the \texttt{acronym} package\footnote{\url{https://ctan.org/pkg/acronym}}.




%Autonomous driving system

%In order to react in time to input, autonomous driving systems need to efficiently process 

%By removing human error and utilizing the faster reaction times of computers, autonomous driving can make transportation safer, more accessible, improve traffic efficiency, and reduce the number of traffic accidents.

%For \ac{AVs}, driving tasks rely on complex computations to both understand the surrounding environment and to react to it in real-time.
%Many of these computations are scheduled onto a \ac{\ac{GPU}}, to leverage the high degree of parallelism in the architecture. 
%However, similar to \ac{CPU}s, the \ac{\ac{GPU}} can experience contention when multiple tasks compete for processing power. 
%Unlike \ac{CPU}s, though, \ac{\ac{GPU}}s are less efficient at quickly switching between different tasks, which can lead to delays and unpredictability. 
%If too many tasks get scheduled at once, the system's ability to meet critical deadlines can be comprimised, potentially leading to safety hazards. 
%Using coroutines on persistant threads, \ac{\ac{GPU}} scheduling can be adapted to mitigate contention and ensure real-time system guarantees, enhancing the safety and reliability of \ac{AV}s.


%These autonomous driving modules use vast amounts of data for their computation tasks, which are based in deep learning, a machine learning technique that demands vast amounts of computational resources.
%Deep learning, inspired by the structure of the human brain, utilizes neural networks, with many layers of neural nodes, to automatically extract patterns, enabling decision-making tasks such as classification and object detection.  
%In perception, deep learning models, particularily \acs{CNN}s identify objects, lane markings, pedestrians, vehicles, and traffic lights from the raw sensor data. 
%Beyond perception, deep learning enhances localization by refining sensor fusion techniques, improving accuracy in determining the vehicles position. 
%Furthermore, deep learning also supports planning and control, through reinforcement learning and predictive models to optimize trajectories. 
%The real-time nature of these components are needed to reduce the latency, which requires these models to continously be updating their predictions as new sensor input arrives and the vehicle moves.
%This places a high demand on the hardware, requiring high-performance accelerators to process vast amounts of data efficiently. 
%To meet these requirements, these computations are scheduled on to a \ac{GPU}, which provides the high degree of parallelism required by the vast amount of data and computations within autonomous driving. 



Real time systems, such as autonomous driving, are designed with strict timing constraints in mind, to ensure predictable and deterministic behavior \cite{10155700}. 
They use a concept of deadlines, which are subdivided into soft and hard-deadlines. 
Hard deadlines are critical and a failure leads to the systems failure or unsafe conditions. 
For example, in autonomous driving, collision avoidance with another vehicle and brake activation are hard deadlines. 
If these deadlines are not met, the safety of the passengers and the system is at risk. 
On the other hand, soft deadlines are not critical and missing these deadlines degrades performance, but does not cause system failure. 
In autonomous driving, this would show in route planning and navigation updates, where a delay would lead to suboptimal paths, but safety is not comprimised. 
Real time systems need to be capable of effectively and efficiently switching from lower priority tasks, soft deadline tasks, to high priority, hard deadline tasks, to ensure the safety both for the passengers and nearby individuals.  
