% !TeX root = ../main.tex

\chapter{Design and Implementation Requirements}\label{chapter:Persistent Threads}

Persistent GPU threads allow the hardware resources to be partitioned and reduce kernel execution and scheduling overhead and are fundamental for other applications seeking to enhance GPU scheduling.  
Many other applications such as coroutines or mega kernels are based on persistent threads as a means of reducing kernel launch overhead and allow further control in application specific scheduling decisions, otherwise black boxed by the cuda api. 
Running persistent threads essentially allows the memory and system configurations to be loaded in ahead of runtime and reduces overhead during execution.

Failing to implement LuisaCompute's coroutines into Apollo, the goal shifted to finding a manual GPU scheduling functionality to implement and use. 
In restricting the scope from GPU coroutines to simply implementing persistent threads into Apollo, this thesis attempted to find an open source implementation which would allow the fine grained scheduling controll specific to Apollo. 
Unfortunately, given that most persistent thread implementations are highly specific they are not open source, which led to selecting an implementation that required more work to make it feasible. 
The only open source persistent thread implementation that was readily available was LightKer, a research project, which measured the hypothetical speedup of using persistent threads over sequential kernel launches. 
Seeking to implement LightKer into Apollo first required a complete restructuring of the code base to support a real time system.

The LightKer implementation itself intended to measure the performance difference between sequential kernel executions versus a persistent kernel implementation.
This implementation constructed simple trivial kernels and tested the overhead difference between calling them explicitely from the host in kernel launches versus implicitely in the persistent kernel. 
Unfortunately, this application does not support variable tasks or memory transfers at runtime, necessary to pass arguments and results back and forth. 
As such, the implementation only succeeds in measuring latency differences between scheduling tasks using a device side while loop versus explicit kernel launches. 
However, the LightKer implementation does provide a simple framework for using a GPU-Host mailbox for scheduling kernel tasks as well as a helpful persistent kernel launch structure. 

\section{Architecture}

From a high level, the architecture implemented into this project appears as follows, with the GPU-Host Mailbox, being used from the Lightkernel project as well as the internal structure.

\begin{figure}[H]
  \centering
  \resizebox{1.0\linewidth}{!}{
	  \input{figures/architecture.tex}
  }
  \caption{Persistent Thread Architecture implemented into LightKer}
  \label{fig:architecture}
\end{figure}


At a high level, there are three important components: the task queue, memory buffers, and the persistent threads themselves.
The task queue is managed, controlled, and allocated by the host and provides the arguments and tasks for the GPU to execute. 
The implementation of the task queue gives the programmer fine-tuned implementation oppertunities to manually design and schedule workloads. 
The memory buffers provide an epoch based staging area for input arguments and output results as well as persistent memory for the further extension of coroutines.
Lastly, the persistent threads are the fundamental execution units executing the \acs{GPU} code.  

The Figure~\ref{fig:architecture} depicts these individual components in a logical program architecture overview.
The left side of the graphic shows the host side code and methods, which allow the enqueuing of tasks and launching of the persistent thread kernel.  
On the right hand side of the graphic, the actual device side code allocation and kernel execution is depicted which allows the processing of memory and write back of results to the memory buffers.
The mailboxes are constantly enqueuing and dequeuing new tasks throughout the execution of the kernels.

\section{Implementation}

GPU kernels are essentially device side functions launched by the host process, which execute with parameters and a logical grid of threads to be distributed across the \acsp{SM}.
In order to replicate both the parameterization and execution model without explicitely launching new kernels, the persistent threads must maintain a mechanism for storing function pointers and their associated arguments to assign work to idle compute resources dynamically.
Similar to other persistent thread implementations, this project implements a task queue, that captures the full execution context required to execute the GPU code. 
As shown in the Figure~\ref{fig:architecture}, the task queue is represented as an array of clipboards each encapsulating a function context.
Beneath the task queue, Figure~\ref{fig:architecture} also depicts the staging area used for memory management. 
The memory management is implemented using an epoch based allocation strategy.
These memory buffers serve as a real-time memory management structure for GPU kernels, reducing the need for direct runtime memory allocation while enabling in-place memory reuse for tasks.

Based on the results in Figure~\ref{fig:singlethreadedgraph}, any sequential scheduling mechanism executing directly on the device would incur significantly performance penalties when compared to a CPU based approach.
As a result, both the task queue and the associated memory buffers are managed from the host side.
This includes deteriming the active epoch and assigning task indices within the corresponding memory or queue structures.
The host is responsible for copying all required parameters into device memory and delegates only the  dequeuing and execution of tasks to the GPU.


Furthermore, standard GPU kernel launches specify the thread configuration and grid layout of GPU threads executing the code and their physical placement in the architecture. 
The GPU automatically decides the execution placement of the kernels from the Gigathread Engine during the launch of GPU code from the host. 
As the persistent threads are already launched at program start, the configuration remains the same throughout the lifetime of the persistent thread. 
Therefore these persistent threads can not support variable launch configurations at runtime without terminating the kernel and restarting a new kernel with different launch configurations.
However, multiple different persistent kernels can be started with various kernel launch configurations, each with different task queues, or through code refactoring the kernels can be adapted to the existing GPU thread block organization.

\subsection{Task Queue}

The task queue functions as a cache like buffer between the host and the executing GPU code, enabling the asynchronous enqueuing of tasks.
Rather than relying on the cuda driver to dynamically partition the GPU and assign tasks to threads, task parameters are instead written into a pre-allocated memory region.
This memory acts as a staging area and remains allocated throughout the duration of the persistent thread kernel, with periodic cleanup. 

Each task entry defines its execution context through an explicit function identifier and its associated parameters.
This entry acts exactly like a coroutine continuation, which stores the necessary state to resume execution within the function. 
The GPU kernel, launched with persistent threads, enters a loop in which each block repeatedly dequeues and executes tasks. 

Within this loop, each GPU block retrieves the next task from the queue, processes it, and stores its results. 
The task queue maintains both input and output buffer offsets for each tasks, allowing blocks to fetch parameters and write results. 
When a task is selected for execution, its parameters are deserialized from the input buffer, converted into an internal representation, and used to invoke the corresponding device function. 

Upon completion of the task, the \acs{GPU} block writes the results to the specified output buffer offset.
This scheme works because the \acs{CPU} knows the exact size of the input and output arguments, knowledge it already must have for issuing cudaMemcpy operations.

Once a the block finishes executing a task, it signals the host that the task is complete and then continues processing the next available task in the queue.

\subsection{Function Pointers}

To execute new functions from the persistent threads, the task queue needs to be able to reference the specific function. 
Generally referencing functions on a \acs{CPU} requires only the function pointer to execute the code defined at that memory location. 
When GPU functions are compiled, the device code lives in the GPU address space and is not accessible from the CPU.  
The CPU only has access to functions denoted by the \lstinline[language=cuda]`__global__` keyword, which allows the execution of GPU kernels, not enqueuing of GPU functions.
In order to be able to access and run the functions specified by the CPU, the task queue supports a lookup table to map integers to specific functions.  
The lookup table allows the host to memcpy in function ids to the task queue when enqueuing new tasks.


\subsection{Function Parameters}

When the CPU assigns tasks to the GPU, it passes either allocated GPU memory pointers or explicit parameters. 
These explicit parameters then get propogated to all the individual threads executing the kernel code, resulting in greater api memory overhead. 
When enqueuing new tasks to the task queue, the memory has to be transfered at runtime before the device function calls. 

The GPU task in the queue originally had a pointer to the allocated memory and upon recieving compute resources would schedule the task with the memory to the individual persistent thread.
Unfortunately, this method is dependent on the specific task and parameters and consumes variable memory requiring further pointers to GPU memory.  
In order to consolidate the memory pointers, the task queue was simplified to contain only allocated memory pointers in order to automatically load kernel memory. 


In this method, enqueing the GPU tasks forces the programmer to streamify the data and automatically load the memory into preallocated memory partitions. 
The task queue then only consists of the actual memory partition pointers, both start and end. 
Executing a task then requires the interpretation of the memory and then the loading of it into the device function.
Manually extending this method allows the user to manually allocate more memory than is needed and use that memory to yield and run coroutines.


\subsection{Memory Model}

In order to provide the incoming scheduled kernels a staging area for allocated memory, the implementation contains a running epoch memory model. 
The memory model contains n epochs, with an input and output staging area for each respective epoch. 
As input are copied into epochs, eventually the corresponding input memory buffer will overflow. 
The host enqueuing functionality manages the current epoch and the current offset within that epoch, which allows the host to detect when the input buffer will lead to an overflow.  
Once the buffer is saturated, the host proceeds to allocate further memory to the next epoch.
After the device finishes executing the task and copies results to the corresponding epoch's result buffer, the host is notified and that memory is marked as free.
After fully allocating the memory for an individual buffer, that buffer remains untouched by the scheduler until the scheduler loops around the entire buffer queue and reaches the same epoch again.


The motivation for this epoch based strategy both asynchronously manages the execution of tasks as well as overhead reduction of continuously freeing and allocating new memory.
As these persistent buffers remain allocated throughout the entirety of the kernel, there is no longer any overhead involved in gpu side memory allocation or freeing, as the memory is tied to the lifetime of the kernel and only internaly considered allocated or freed.  
When the buffer queue loops and returning back, if the specific buffer parameters and memory size has been correctly set, potentially through profiling, the buffer will be free and can be reused.
The buffer is only considered free if all tasks within the buffer are considered free, which removes any complicated GPU side memory allocation schemes.

\subsection{Cuda Streams Optimization}

To fully leverage the GPUâ€™s memory transfer capabilities, input and output transfers are performed in separate, overlapping CUDA streams.
As previously noted, the GPU features distinct engines for \acs{H2D} and \acs{D2H} transfers, which can operate concurrently. 
By assigning input transfers to a dedicated H2D stream and output transfers to a separate D2H stream, this implementation avoids intra-stream dependencies and enables parallel data movement in both directions.


