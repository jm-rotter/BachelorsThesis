% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.


\section{Objectives and Contributions}


\subsection{Original Objective}

The original objective of this thesis was to integrate an existing coroutine-based GPU scheduling framework into an autonomous driving system. 
This integration aimed to evaluate the feasibility of fine-grained GPU scheduling within a complex, real-time environment. 
Furthermore, by measuring scheduling latencies in the system, it would be possible to derive and refine strict timing guarantees.

\subsection{Challenges and Scope Adjustment}

Direct implementation of the coroutine framework proved infeasible within the available time frame. 
The system was initially designed for graphics rendering, relying on a complex compiler-based architecture with limited documentation, and required a separate build process. 
Combined with limited prior experience in GPU programming and compiler theory, these factors made integration within the timeline challenging. 

To simplify the problem, the focus shifted from coroutines to the underlying execution model, persistent threads.
No suitable open source implementation was found that could be directly integrated into an autonomous driving system. 
Instead, a minimal, open-source, custom persistent thread scheduler, LightKernel, was found and extended to be suitable for real-time scheduling \cite{lightker}. 
The implementation initially measured only scheduling overheads from kernel launches, but provided a foundation for implementing fully functional persistent threads. 
This new scheduler serves as a proof of concept foundation that implements long-running GPU kernels to receive and execute tasks efficiently.
Furthermore, it provides the framework on which GPU coroutines can be implemented to enable suspension and prioritization between tasks, allowing exploration of how real-time behaviors can be approximated within the constraints of the CUDA execution model.



\subsection{Contributions}

This thesis designs and implements several persistent thread components to enable the efficient execution of GPU code. 

\begin{enumerate}

	\item \textbf{Task Management System:}
		An execution management system was developed to allow tasks to be queued and executed independently of resident executing kernels. 
		This system captures the full execution context of functions, enabling persistent threads to retrieve, schedule, and execute tasks without requiring new kernel launches.

	\item \textbf{Memory Management System:}
		To eliminate memory allocations throughout the runtime of the persistent kernel, a memory management system was designed.	
		Tasks inserted into the task management utilize the memory management systems for their associated memory allocations.
		In this system, memory is mapped into a running epoch buffer of preallocated memory assigned before the launch of the persistent kernel. 
		This system also provides a logical partitioning of input and output buffers to reduce data interdependencies.


	\item \textbf{Concurrent Memory and Execution models:}
		To support concurrent memory transfers between the host and device, as well as simultaneous kernel execution, the system leverages multiple independent CUDA streams to resolve data dependencies.
		This system enables higher overlap between memory and compute operations, resulting in better utilization of GPU resources.

	\item \textbf{GPU Task Coordination and Synchronization:}
		The persistent kernel is spread across multiple thread blocks to execute distinct tasks concurrently.
		This system is synchronized within the device code to ensure that no two blocks execute the same task simultaneously.
		By enforcing exclusive task execution, the mechanism prevents race conditions and ensures correctness across all kernels.
		Furthermore, this system enables the host to schedule efficiently and queue tasks to any available thread block, thereby maintaining high GPU utilization. 

\end{enumerate}


%\section{Thesis Outline}
% TODO
