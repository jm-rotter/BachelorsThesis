% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Introduction}\label{chapter:introduction}
Autonomous driving systems place stringent demands on computational performance, predictability and safety.
These demands arise from the need to process vast amounts of sensor data and run complex perception and decision making algorithms in real time, while ensuring timely and deterministic responses to a dynamic environment.
To meet the computational requirements of such systems, \acsp{GPU} have become essential due to their performance on machine learning workloads. 
However, the current GPU programming and execution model is poorly suited to real time constraints. 

Modern, CUDA capable GPUs, rely on a proprietary hardware scheduler that functions as a black box to the programmer.
This scheduler is optimized for throughput rather than deterministic execution, which introduces unpredictable latency from resource contention between tasks.
Critically, tasks with strict timing requirements may suffer delays if long running, lower priority kernels are already resident on the device. 
The inability to preempt GPU kernels or to enforce strict task priorities makes the utilization of GPUs difficult for safety critical, real time workloads.  
Particularily in the context of autonomous driving, the repercussions of latencies pose a serious safety concern. 

Historically, early autonomous driving systems addressed the real time requirements using a distributed architecture.
In this approach, major functional modules, such as perception, localization, planning, and control, were mapped to seperate compute units, which together formed a processing pipeline \cite{6809196}. 
Each module could thus operate with predictable timing characteristics, avoiding contention with other modules. 
In this manner, the distributed architecture allowed for fine tuning the timing between modules to achieve low latency responses from the hardware. 
This modular architecture ensured responsiveness and real time guarantees at the expense of high hardware cost and increased system complexity.


The rise of increasingly powerful GPUs has enabled a shift toward centralized computing in order to simplify the hardware and complexity while reducing costs. 
In this centralized architecture, all core driving modules share a common compute node consisting of a CPU-GPU system. 
Moving to a singular compute node allows savings in cost, design, and intermodule latency.
This compute node uses the GPU for compute intensive workloads, while leaving the CPU free to orchestrate the scheduling and control of the system. 

Despite the benefits afforded by a centralized architecture and the advances in hardware, the system still risks GPU oversubscription.
As the GPU is simultaneously responsible for all processing tasks, too many simultaneous scheduled tasks can lead to delays in execution time. 
Typical real time system solutions based on a \acs{CPU}, ensure system safety under contention by preempting non critical tasks. 
Tasks requiring high responsiveness can thus be directly executed after preemption of the resident threads and processes.
This preemption is supported at both the OS level as well as the programming level, which this thesis aims to abstract and implement for \acsp{GPU} in a real time autonomous driving system.

To enable the GPU to support these real time features, the device scheduling has to be handled by the programmer instead of natively.
Rather than relying on native kernel launches, this thesis explores GPU coroutines and persistent threads as mechanisms for enabling predictable, low latency execution. 
Coroutines provide cooperative multitasking between GPU threads, allowing the system to make scheduling decisions otherwise restricted by the proprietary driver.
This additional scheduling ability allows the system to implement custom schedulers that prioritize critical work. 
Persistent threads, long lived threads that remain active throughout the lifetime of the system, are fundamental to the management of these coroutines. 
Furthermore, persistent threads have the added benefit of reducing kernel launch overhead. 

As part of this research, an attempt was made to directly integrate a coroutine based GPU scheduling system into an autonomous driving system. 
However, the lack of documentation and time as well as the complexity of the existing framework proved to be a limitation. 
As a result, this thesis pivoted to implementing a custom persistent thread scheduler on which coroutines can be implemented for real time systems.  
This approach enables long running GPU threads to receive new tasks, yield between them, and implement task prioritization in software. 
The software yielding emulates the desired preemption to improve responsiveness under load. 


In summary, this thesis investigates real time GPU scheduling techniques for autonomous driving.
By building a persistent thread architecture to support coroutine based task control, the goal is to reduce latency variability and enable timely execution of safety critical GPU workloads. 
This thesis aims to bridge the gap between the throughput oriented design of modern GPUs and the strict timing guarantees demanded by real time autonomous systems.
