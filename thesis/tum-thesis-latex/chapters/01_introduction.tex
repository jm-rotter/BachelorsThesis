% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Introduction}\label{chapter:introduction}
Autonomous driving systems place stringent demands on computational performance and predictability, yet \acsp{GPU}, needed to process sensor data and run machine learning tasks, can not natively support time critical demands. 
The current CUDA \acs{GPU} hardware scheduler, functioning as a black box, maximimizes throughput rather than deterministic execution, leading to unpredictable latency from resource contention, which poses a serious safety hazard.
The proprietary nature of the hardware scheduler makes it difficult to enforce timing guarantees, prioritize critical tasks, and suspend resident kernels.
In the absence of preemption or fine grained resource control, high priority workloads can be delayed by longer running, lower priority kernels. 
This thesis investigates GPU scheduling strategies tailored to real time systems, focusing on persistent threads as a mechanism to improve responsiveness, reduce latency variability, and ensure the timely execution of safety critical tasks in autonomous driving.

Early autonomous driving systems used a distributed architecture to ensure the timing guarantees of individual modules \cite{6809196}.
The distributed architecture processes the individual driving tasks into the modules perception, localization, planning, and control, which together form a processing pipeline. 
The stages of the pipeline enable the vehicle to interpret its surroundings, determine its position within them, make decision, and execute corresponding actions. 
In this architectural design, each module is mapped to an individual compute unit, where every unit only runs tasks related to their modules.  
Among the benefits of this system design is that the load on the compute units is consistent.
For example, the planning node only ever computes planning related tasks. 
In this manner, the distributed architecture allows for fine tuning the timing between modules to achieve a low latency responses from the hardware. 

Today, as the hardware has become more performant, autonomous driving has become centralized, in order to lower costs and latency between modules.
By centralizing the compute resources, the system uses a singular compute node, which manages all of the tasks simultaneously. 
This structural choice benefits the system design as well as the latency cost as all the tasks are colocated. 
The drawback to the centralized computing node is the contention between tasks, where the execution time is variable depending on the current execution queue.

The original distributed system design was necessary in part due to the vast amount of computations and processing required by the autonomous driving system.
In particular, the core modules of an autonomous driving system each require complex neural nets in order to allow the vehicle to function autonomously. 
Currently, \acsp{CPU} alone are not performant enough to react to the real time constraints imposed by autonomous vehicles.
These constraints are necessary in order to ensure the safety of the passengers, system and enviornment.
To meet these constraints, autonomous vehicles uses a heterogenous computing architecture with specialized chips, such as the NVIDIA \acsp{GPU}, to support the heavy processing tasks required by the system. 
The \acsp{GPU} themselves are not designed to be used in real time systems and using standard real time system algorithms for these chips is infeasible due to the different programming models.

Unfortunately, for the same architectural reason as why \acsp{GPU} excell on compute heavy workloads, they can not guarantee execution latencies. 
The \acsp{GPU} are implemented on a batch system algorithm, where throughput is prioritized above all else. 
Unlike real time systems typically implemented on \acsp{CPU}, GPU kernels do not natively support interruption to allow higher priority tasks to execute \cite{taskparallelism}.
The \acs{GPU} kernels are queued and scheduled based on availability and executed until completion without interruption.
By prioritizing throughput over responsiveness and latency, the \acsp{GPU} may become contented between various different tasks, which leads to variable execution and latency times, dependent on the current and queued loads.
Variable latencies are unacceptable in real time systems where strict execution deadlines must be preserved in order to react to the changing enviornment in time. 
This paper proposes a method to allow the \acs{GPU} to be partitioned, reducing kernel launch latency, and enable the integration of \acsp{GPU} into real time systems.


%Background
