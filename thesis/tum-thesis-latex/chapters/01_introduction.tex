% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Introduction}\label{chapter:introduction}
Autonomous driving presents a formidable challenge in computing, requiring complex high-performance workloads that heavily rely on \acs{GPU}s, yet traditional scheduling techniques fail to deliver the necessary predictability and reliability demanded by real-time systems.
\ac{GPU} hardware is optimized for maximizing throughput, a focus that compromises low-latency performance critical for real-time applications. 
Similarily, traditional \ac{GPU} scheduling methods do not consider the strict deadlines essential for real time systems, particularily under conditions of \ac{GPU} contention, where multiple tasks compete for limited computational resources. 
This thesis will implemented an approach to \ac{GPU} scheduling using coroutines on persistant threads to reduce latency variability and address contention issues, ultimately delivering the predictability and reliability necessary for real-time autonomous driving applications. 

\subsection{Necessesity of \acsp{GPU} in Autonomous Driving}
Recently, \acsp{GPU} have revolutionized data processing with their ability to handle massive amounts of data and execute highly-parallelized computations.
Previous data processing systems, which used traditional CPUs, by contrast, relied on a \ac{MIMD} architecture that excels at handling complex control logic at high frequencies by executing different instructions on seperate data streams concurrently. 
However, for  tasks that require the repeated execution of singular instructions such as large-scale data processing workloads, CPUs are burdened by the overhead of the complex control logic, where a simpler, more parallel design would be more effective.
\acs{GPU}s, created to overcome this limitation, use a \ac{SIMT} architecture with thousands of simpler, slower threads executing simultaneously. 
The parallelism provided by the \ac{SIMT} model allows a far higher throughput that outperforms \acsp{CPU}. 
Here, each data element is processed by its own thread, allowing the same instruction to be executed simultaneously across numerous data streams.


Higher data processing performance is vital to autonomous systems, which rely extensively on machine learning tasks and sensor data processing. 
In autonomous systems a multitude of modules require deep learning, a machine learning technique, to automatically extract patterns, such as computer vision, and make decisions. 
Deep learning, inspired by the structure of the human brain, is composed of layers of numerous interconnected, identical nodes called neural networks.  
Neural network models rely on mathematical operations between different neighboring layers to perform inference, essentially the prediction making or recognition process.
The layers are represented as matrices, which then get multiplied and convoluted with one another and are used by specialized functions, called activation functions, to introduce non-linearity. 
With very large neural networks with thousands or millions of nodes, the inference computation is repetitive and slow. 
The capability to execute these repeptitive workloads concurrently across different dataset makes \acsp{GPU} fundamental to performance in tasks using complex neural networks.  
Furthermore, \acsp{GPU} are necessary to process the data rate produced by high-bandwidth, high-frequency sensors used in autonomous driving. 
The parallelism in \acs{GPU}s makes them the ideal platform over CPUs for deep learning and data processing tasks, tasks fundamental for autonomous driving systems. 

\subsection{Autonomous Driving as a Real Time System}
Real time systems, such as autonomous driving, are designed with strict timing constraints in mind, to ensure predictable and deterministic behavior. 
They use a concept of deadlines, which are subdivided into soft and hard-deadlines. 
Hard deadlines are critical and a failure leads to the systems failure or unsafe conditions. 
For example, in autonomous driving, collision avoidance with another vehicle and brake activation are hard deadlines. 
If these deadlines are not met, the safety of the passengers and the system is at risk. 
On the other hand, soft deadlines are not critical and missing these deadlines degrades performance, but does not cause system failure. 
In autonomous driving, this would show in route planning and navigation updates, where a delay would lead to suboptimal paths, but safety is not comprimised. 
Real time systems need to be capable of effectively and efficiently switching from lower priority tasks, soft deadline tasks, to high priority, hard deadline tasks, to ensure the safety both for the passengers and nearby individuals.  

\subsection{GPU weaknesses in Real Time Systems}
Given the reliance on \acsp{GPU} in autonomous driving systems for data processing and machine learning, they need to meet the strict timing requirements of real-time applications, which is lacking under current scheduling methods.
GPU tasks are traditionaly queued and scheduled based on availabilty to optimize for high throughput applications like graphics rendering and offline machine learning training.
These tasks run to completion, similar to a batch system, meaning multiple tasks, such as the different modules in autonomous driving execute sequentially. 
Thus, depending on the current workload for the GPU, the execution time and latency can vary, which poses a safety risk.
To adapt \acsp{GPU} to the requirements of real time systems, while maintaining the advantages they provide in processing power, the scheduling of these systems needs to support asynchronous programming.


\section{Background}
\subsection{Asynchronous Programming and Coroutines}

Asynchronous programming is a method of programming a system to handle tasks concurrently instead of sequentially. 
Typically used in conjunction with tasks that delay or have high wait-times, such as I/O heavy jobs, asynchronous programming reduces overall execution time by more efficiently using processing ressources. 
For example, while waiting for I/O heavy input from sensor input, asynchronous code lets other tasks execute in the meantime, before returning when the data arrives.  
For real-time systems asynchronous programming furthers the use of intermittant execution to enforce determinism. 
By allowing the \acsp{GPU} to switch between concurrent tasks, hard deadlines can be immediately enforced without delay. 



Coroutines, an implementation of asynchronous programming, uses suspendable functions to halt execution. 
Suspendable functions are implemented by capturing the current context, know as the continuation, of a function and process and saving it. 
After being saved, a new process can take over execution, without interrupting the state of the previous process. 
Once the intermittant process or higher priority process has finished execution, the first task can continue executing by restoring the process context. 
Capturing the continuation of a function allows the resumption of the program to be strategically deferred \cite{Zheng2022LuisaRender}. 

coroutines
Each tasksk
Here, the scheduled tasks are independent of the control flow, meaning the execution order of the subtasks



\subsection{Autonomous Vehicles}
Autonomous driving systems, such as self-driving cars, are subdivided into several modules, perception, localization, mapping, planning and control, which all rely on massive data processing and machine learning, to help the system to understand its enviornment, plan its trajectory and drive without any human help. 
Perception gathers and interprets vast amounts of data from numerous sensors, such as \ac{LiDAR} sensors, cameras, radar, and ultrasonic sensors, to identify and classify surrounding objects.
After perception, localization and mapping use a \ac{SLAM} algorithm to build maps and determine the vehicle's precise location within its enviornment and the generated map. 
Using the foundation set by localization and mapping, path planning determines the vehicles trajectory from its current location to a target destination, while respecting traffic laws and avoiding obstacles. 
Lastly, control actively follows the trajectory and sets the speed, steering and braking required for the planned path.
Autonomous driving sytems are constantly computing each of these modules based on updates in the surroundings, which requires significant computational ressources to maintain a low latency. 

Specifically, these modules require large amounts of data to accurately compute , which are based in deep learning, a machine learning technique that demands vast amounts of computational resources.
In perception, deep learning models, particularily \acs{CNN}s identify objects, lane markings, pedestrians, vehicles, and traffic lights from the raw sensor data. 
Beyond perception, deep learning enhances localization by refining sensor fusion techniques, improving accuracy in determining the vehicles position. 
Furthermore, deep learning also supports planning and control, through reinforcement learning and predictive models to optimize trajectories. 
The real-time nature of these components are needed to reduce the latency, which requires these models to continously be updating their predictions as new sensor input arrives and the vehicle moves.
This places a high demand on the hardware, requiring high-performance accelerators to process vast amounts of data efficiently. 
To meet these requirements, these computations are scheduled on to a \ac{GPU}, which provides the high degree of parallelism required by the vast amount of data and computations within autonomous driving. 




\subsection{Asynchronous Programming}





\subsection{Real Time Systems}

\subsection{Autonomous Driving Systems}



\subsection{\acs{GPU}s}


\subsection{NVIDIA Tesla V100 Architecture}

For the purpose of this thesis, the NVIDIA Tesla V100 accelerator, which uses the Volta GV100 \ac{GPU} architecture, was selected due to its availability and high-performance computing capabilities.
At the core of its execution model is the warp, a group of 32 threads executing in \ac{SIMT} fashion.
Each warp is scheduled and dispatched within one of the 4 processing partitions of a \ac{SM}. 
The warp scheduler and warp dispatcher each handle 32 threads per clock cycle. 
Within each \ac{SM} partition, there are Tensor Cores for deep learning, 64 bit-\ac{FP} cores, \ac{LD/ST} units, a register file, and \acs{SFU}s for mathematical functions such as  sine and square root. 
Additionally, each partition contains 16 Cuda cores, which can execute both \ac{FP} 32 bit and \ac{INT} 32 bit instructions, but not simultaneously. 
Each partition has its own L0 instruction cache, while all partitions share a L1 instruction and data cache. 
A \ac{SM} can support up to 64 concurrent warps, allowing a maximum of $64 * 32 = 2048$ threads per \ac{SM}. 
At a higher level, two \acs{SM}s are grouped to form a \ac{TPC}. 
The Tesla V100 GPU consists of six \acs{GPC}s, each containing seven \acs{TPC}s, resulting in a total of 84 \acs{SM}s across the chip. 
This hierarchical organization, from warps to \acs{SM}s, \acs{TPC}s, and \acs{GPC}s, enables the Tesla V100 to efficiently handle massively parallel workloads, making it well-suited for high-performance computing and deel learning applications. 

\section{Motivation}
In autonomous driving and other real time system tasks, GPUs are essential for data processing and machine learning; however,  existing GPU scheduling techniques fail to meet the strict timing requirements of real-time applications.
GPU tasks are traditional queued and scheduled based on availabilty to optimize for high throughput applications like graphics rendering and offline machine learning training.
These tasks run to completion before switching tasks, meaning multiple tasks, such as the different modules in autonomous driving cannot efficiently share resources. 
Depending on the current workload for the GPU, execution times and latency can vary. 
These limitations make existing GPU scheduling mechanisms unsuitable for real-time applications like autonomous driving, where deadlines need to be met to ensure the safety of the system and passengers. 

%Citation test~\parencite{latex}.

%Acronyms must be added in \texttt{main.tex} and are referenced using macros. The first occurrence is automatically replaced with the long version of the acronym, while all subsequent usages use the abbreviation.

%E.g. \texttt{\textbackslash ac\{TUM\}, \textbackslash ac\{TUM\}} $\Rightarrow$ \ac{TUM}, \ac{TUM}

%For more details, see the documentation of the \texttt{acronym} package\footnote{\url{https://ctan.org/pkg/acronym}}.




%Autonomous driving system

%In order to react in time to input, autonomous driving systems need to efficiently process 

%By removing human error and utilizing the faster reaction times of computers, autonomous driving can make transportation safer, more accessible, improve traffic efficiency, and reduce the number of traffic accidents.

%For \ac{AVs}, driving tasks rely on complex computations to both understand the surrounding environment and to react to it in real-time.
%Many of these computations are scheduled onto a \ac{\ac{GPU}}, to leverage the high degree of parallelism in the architecture. 
%However, similar to \ac{CPU}s, the \ac{\ac{GPU}} can experience contention when multiple tasks compete for processing power. 
%Unlike \ac{CPU}s, though, \ac{\ac{GPU}}s are less efficient at quickly switching between different tasks, which can lead to delays and unpredictability. 
%If too many tasks get scheduled at once, the system's ability to meet critical deadlines can be comprimised, potentially leading to safety hazards. 
%Using coroutines on persistant threads, \ac{\ac{GPU}} scheduling can be adapted to mitigate contention and ensure real-time system guarantees, enhancing the safety and reliability of \ac{AV}s.


%These autonomous driving modules use vast amounts of data for their computation tasks, which are based in deep learning, a machine learning technique that demands vast amounts of computational resources.
%Deep learning, inspired by the structure of the human brain, utilizes neural networks, with many layers of neural nodes, to automatically extract patterns, enabling decision-making tasks such as classification and object detection.  
%In perception, deep learning models, particularily \acs{CNN}s identify objects, lane markings, pedestrians, vehicles, and traffic lights from the raw sensor data. 
%Beyond perception, deep learning enhances localization by refining sensor fusion techniques, improving accuracy in determining the vehicles position. 
%Furthermore, deep learning also supports planning and control, through reinforcement learning and predictive models to optimize trajectories. 
%The real-time nature of these components are needed to reduce the latency, which requires these models to continously be updating their predictions as new sensor input arrives and the vehicle moves.
%This places a high demand on the hardware, requiring high-performance accelerators to process vast amounts of data efficiently. 
%To meet these requirements, these computations are scheduled on to a \ac{GPU}, which provides the high degree of parallelism required by the vast amount of data and computations within autonomous driving. 
