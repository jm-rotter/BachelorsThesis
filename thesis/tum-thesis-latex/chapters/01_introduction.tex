% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Introduction}\label{chapter:introduction}

\section{Motivation}
Autonomous driving systems place stringent demands on computational performance, predictability and safety.
These demands arise from the need to process vast amounts of sensor data and run complex perception and decision making algorithms in real time, while ensuring timely and deterministic responses to a dynamic environment.
To meet the computational requirements of such systems, \acsp{GPU} have become essential due to their performance on machine learning workloads. 
However, the current GPU programming and execution model is poorly suited to real time constraints. 
While this thesis does not implement the proposed approach directly within an autonomous driving stack, the challenges of meeting real time requirements in autonomous systems, particularly in GPU workloads, serve as the primary motivation for this research.


\subsection{Evolution of Autonomous Driving Architectures}
Historically, early autonomous driving systems addressed the real time requirements using a distributed architecture.
In this approach, major functional modules, such as perception, localization, planning, and control, were mapped to seperate compute units, which together formed a processing pipeline \cite{6809196}. 
Each module could thus operate with predictable timing characteristics, avoiding contention with other modules. 
In this manner, the distributed architecture allowed for fine tuning the timing between modules to achieve low latency responses from the hardware. 
This modular architecture ensured responsiveness and real time guarantees at the expense of high hardware cost and increased system complexity.


The rise of increasingly powerful GPUs has enabled a shift toward centralized computing in order to simplify the hardware and complexity while reducing costs. 
In this centralized architecture, all core driving modules share a common compute node consisting of a CPU-GPU system. 
The compute node integrates a CPU for task scheduling and system control, complemented by a GPU optimized for compute intensive workloads.
Moving to a singular compute node allows savings in cost, design, and intermodule latency.

\subsection{Limitations of Current GPU Execution Models}

Despite the benefits afforded by a centralized architecture and the advances in hardware, the system risks GPU oversubscription.
As the GPU is simultaneously responsible for all processing tasks, too many simultaneous scheduled tasks can lead to delays in execution time. 
Typical real time system solutions based on a \acs{CPU} architecture, ensure system safety under contention by preempting non critical tasks. 
Tasks requiring high responsiveness can thus be directly executed after preemption of the resident threads and processes.
This preemption is supported both natively at the OS and user space levels on the \acs{CPU}, but not the \acs{GPU}. 

Modern GPUs, are unable to natively support real time programming models and instead rely on a proprietary hardware scheduler that restricts the programmer's scheduling control.
The scheduler in particular is optimized for throughput rather than deterministic execution, which is required by real time systems. 
Critically, tasks with strict timing requirements may suffer delays if long running, lower priority kernels are already resident on the device. 
The inability to natively preempt GPU kernels or to enforce strict task priorities makes a native utilization of GPUs difficult for safety critical, real time workloads.  
Particularily in the context of autonomous driving, the repercussions of latencies pose a safety concern. 


\section{Problem Statement}

Rather than relying on native kernel launches, this thesis investigates the use of persistent threads to enable low latency execution, and develops the foundational framework needed to integrate coroutines into the system.
Persistent threads are specialized kernels that are launched at the start of the application and remain active throughout the lifetime of the system. 
They function as a user level scheduler, continuously polling for work, executing tasks, and executing scheduling decisions. 

Building on this framework, the central research question addressed in this thesis is:

\vspace{\baselineskip}

\textbf{How can a persistent GPU thread model be designed to support the implementation of GPU coroutines for predictable, low latency scheduling on real time systems?}

\vspace{\baselineskip}

As part of this research, the following aspects will be considered and evaluated:

\begin{itemize}
    \item \textbf{GPU Stream Management:} Organizing and scheduling multiple GPU streams to enable concurrent task execution with memory transfers while maintaining predictable execution orders.
    
    \item \textbf{GPU Device Memory Management:} Efficient allocation, deallocation, and usage of GPU memory to ensure low latency access and avoid contention between tasks.
    
    \item \textbf{GPU Task Enqueuing and Dequeuing:} Mechanisms for submitting tasks to the GPU and retrieving results asynchronously.
    
    \item \textbf{Framework for Coroutines:} Designing a foundation for GPU coroutines that allows cooperative multitasking and the implementation of custom scheduling strategies on top of persistent threads.
\end{itemize}





