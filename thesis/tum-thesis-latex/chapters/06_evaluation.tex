% !TeX root = ../main.tex

\chapter{Experiments and Evaluation}\label{chapter:Evaluation}

The evaluation focuses on measuring the model’s effectiveness in meeting the stated objectives as defined in Chapter~\ref{chapter:introduction}.
In particular, it examines the reduced scheduling latency and kernel overhead when invoking kernel functions.
The results highlight both the benefits of the approach and its limitations.


\section{Limitation in Memory Transfers during Evaluation}

During testing, the evaluation revealed nondeterministic behavior in the execution results.
In some runs, the kernels scheduled did not use the correct input matrices but instead used uninitialized memory, resulting in output matrices containing only zeros.
Under further testing, this issue never occurred when debugging print statements were inserted, suggesting that the nondeterministic issue was timing-related.
In particular, the input arguments were being processed before they were available. 
While this issue was not addressed in the current implementation, documenting it is important to understand the limitations and potential challenges associated with using persistent GPU threads for real-time workloads.

Given that the print statements ensured correct execution and the values inside the kernel functions were observed to be incorrect, the issue lies with the enqueuing of tasks to the GPU. 
The task enqueuing process first copies memory, then copies the task into the respective buffers within the same stream, and only afterward notifies the GPU of the new functions.
Therefore, the memory should be consistent and correct before the task is even in the buffer to be executed. 
However, the GPU periodically polls for new tasks after a deadline to avoid missing updates if the CPU enqueues a task while the message flag is reset.
When this polling occurs as the task is being copied into the memory buffer, it is possible that a memory copy is completed only partially, resulting in correct execution values but an incorrect offset. This can lead to incomplete or invalid matrices being processed.

This hypothesis remains unproven; however, in cases with incorrect values, kernel execution times are significantly reduced, which can lead to incomplete analysis of the results. 
Therefore, the evaluation in the following graphs contains only the execution runs that correctly computed all values within the task queue.

\section{Experimental Setup}

To assess the effectiveness of persistent threads in reducing scheduling latency, the solution was tested against a baseline implementation using a matrix multiplication benchmark. 
For this analysis, a simple random matrix multiplication task was repeatedly generated and scheduled 32 times using the persistent thread implementation.
The measured latency was then compared to a straightforward kernel invocation. 
All experiments were conducted on an NVIDIA Tesla V100 GPU running CUDA version 11.8.

\subsection{Matrix Multiplications for Performance Testing}

The current execution model requires manual task wrappers for scheduling and executing GPU tasks, which adds complexity when supporting a variety of functions. 
These wrappers reduce the overhead of GPU memory allocation, but require serialization and deserialization of the data during execution.
To keep testing manageable, this thesis focuses on matrix multiplications as a representative example to demonstrate the solution's capabilities. 

In particular, the benchmark uses matrix multiplications of a fixed size, 16 x 16, chosen due to the inflexibility of changing configurations during the kernel's lifetime. 
Typically, the programmer specifies kernel thread configurations at launch; however, this is not possible with persistent threads.
Their configuration is fixed at launch and cannot be changed during execution.
In practice, such values need to be determined by profiling the workload and implementing them manually.
Since performance improves when the number of threads matches or exceeds the number of elements in the solution matrix, this implementation uses a matrix with a 16×16 grid dimensionality.

\subsection{Simulated Workload for Testing}

Because the benchmark and persistent kernel implementations differ in their application goals, a simulated workload was designed for testing purposes. 
In these tests, the persistent kernel task schedules 32 random matrix multiplication tasks to the GPU and then immediately begins waiting for results. 
As the tasks are being scheduled, the GPU concurrently executes already enqueued tasks. 
After completing individual tasks, the host is notified via the mailbox message. 
By contrast, the matrix multiplication benchmark executes tasks serially in an allocation, memory copy, kernel launch, and memory copy back loop.  


\subsection{Latency Measurements}


\begin{figure}[H]
  \centering
  \input{figures/eval_sm_sk.tex}
  \caption{Small Kernel and Memory Transfer Tasks}
  \label{fig:sm_sk}
\end{figure}


Figure~\ref{fig:sm_sk} presents the first experiment, comparing task scheduling using the GPU persistent kernel against the baseline model. 
Here, each implementation executes 32 16x16 matrix multiplication tasks scheduled to the GPU. 
The matrices for testing are generated at runtime and compared to the correct CPU implementation using the same functions.  

Across 10,000 runs, the persistent kernel averaged achieved an execution time of 207.12 ms, compared to 229.60 ms for the baseline model. 
This demonstrates a clear performance advantage for the persistent kernel in cases where small kernels are used with small memory transfers. 
However, the measurements exhibited high variability, with execution times differing by as much as $\pm$50 ms between runs, even when averaged over 10,000 executions.
Despite this variability, the persistent kernel consistently outperformed the baseline in trials.


\subsection{Latency Measurements with Large Memory Transfers}

\begin{figure}[H]
  \centering
  \input{figures/eval_bm_sk.tex}
  \caption{Small Kernel and Larger Memory Transfer Tasks}
  \label{fig:bm_sk}
\end{figure}

In this secondary evaluation, the GPU executes the same task as before but with an additional 1 MiB of memory transferred to and from each task.
Interestingly, the persistent thread implementation ran slightly faster than in the previous test, while the baseline control became significantly slower.
For reference, the persistent kernel completed in 178 ms, compared to 350 ms for the non-persistent version.
This unexpected outcome prompted further testing with a larger input size of 10 MiB for the persistent kernel implementation, which completed in 623.915ms.

The performance improvement is likely attributable to the underlying memory management strategy. 
To conserve GPU resources, memory buffers are partitioned logically using offsets.
When tasks use very small memory frames, their data is placed closely together in memory.
During execution, this proximity can lead to contention: tasks from memory locations adjacent to those being written to by other tasks may experience reduced performance as memory operations are serialized.
By increasing the memory size transferred per task, the allocated regions are spaced farther apart, reducing contention and allowing the kernel to execute more efficiently.

\section{Multiple GPU Blocks}

Unfortunately, particularly in the case of running multiple GPU blocks, the memory copying errors were observed with a much higher frequency. 
As the GPU blocks were polling more frequently and able to consume tasks more quickly, they had faulty, uninitialized parameters.  
The results from this test failed too often to support any significant data. 


\chapter{Conclusion and Future Work}


This work presents a GPU persistent thread model, designed both as a foundation for future research in scheduling and as a way to improve the efficiency of executing multiple tasks on the GPU by reducing kernel launch overhead.
The model supports application-specific fine-tuning, with particular focus on managing hardware resources.
In this approach, the persistent kernel runs throughout the application's lifetime, dynamically processing incoming tasks streamed to the GPU and avoiding repeated kernel launches.

The implementation meets the objectives established for developing a real-time GPU scheduler.
In particular, it demonstrates that persistent threads reduce scheduling overhead and improve latency. 
Furthermore, Chapter~\ref{chapter:Persistent Threads}  outlines how priorities and coroutines could be integrated into this persistent thread model. 
Overall, this work enhances understanding of real-time GPU scheduling, CUDA architecture, and GPU execution and programming models, providing a foundation for future improvements and research in high-performance real-time GPU computing.

\section{Limitations}

\subsection{Input Parameter Memory Limitation}

The primary limitation observed is the non-deterministic results discussed in Chapter~\ref{chapter:Evaluation}. 
This behavior arises from the way tasks are enqueued.
While the GPU should manage tasks to provide support for fine-grained scheduling, it must also ensure data consistency.
In this implementation, the GPU needs to decouple memory transfers and context information from the actual execution logic.
Once the transfer and context information has been transferred, then the action can be executed. 

This could be addressed in several ways to synchronize the GPU with the CPU further.
Currently, the synchronization mechanism of GPU mailboxes only supports a single mailbox. 
Instead, the implementation could support a mailbox per task element or one per \acs{SM}, allowing GPU \acs{CTA} blocks to constantly synchronize on these tasks, rather than on the tail of the queue. 
Furthermore, this approach enables the GPU to assign tasks to specific partitions, thereby reducing interference. 
Another approach is to introduce a separate execution buffer that only allows task execution after all necessary data has been copied and made stream-coherent.
Lastly, the GPU could track the number of tasks waiting for execution using a synchronization method, which is only updated after tasks are coherent. 


\subsection{Persistent Kernel Limitations}

The persistent kernel scheduler is intended to work in harmony with the internal hardware scheduler.
Conflicts between the different schedulers can negatively influence the performance or result in a failed kernel launch. 
For example, in this thesis, the initial evaluation was performed on larger matrices, which worked well for benchmarking the implementation but failed to execute on the GPU persistent thread implementation due to register pressure.
Furthermore, thread blocks can be inoptimally managed and scheduled by the user level scheduler to resource contented SMs.
In such cases, theinternal handler would have been better.
Consider the case of scheduling two tasks that are heavy in shared memory to the same SM, which would lead to contention and reduced performance.
Attempting to schedule tasks manually requires a detailed understanding of both hardware profiling and resource usage, making the approach substantially more complex.

When optimizing the persistent kernel implementation using NVIDIA Nsight Systems (nsys), the profiling process also became more challenging.
Nsys provides detailed kernel execution latency breakdowns of kernels.  
However, because the persistent kernel itself executes the tasks, it becomes challenging to identify which task is currently running and to measure its latency.
As a result, task latencies must be measured explicitly, either through CUDA events or with custom tracking mechanisms.

Lastly, in this implementation, the custom wrappers written for serializing and deserializing parameters introduce further complications.
Supporting a wide range of tasks required significant implementation effort, and these wrappers added extra overhead.
This not only slowed down the system but also increased the complexity for the user, making the approach more labor-intensive compared to an automatic solution.



\section{Future Work}
The current system does not support priority-based scheduling decisions or the use of coroutines for software preemption.
Future work could build on the strategies outlined in this thesis to extend the persistent kernel implementation with these features, enabling more predictable real-time scheduling. 
In this case, the memory management system may need to evolve from an epoch-based scheduler to a more complex management system. 
Additionally, the current manual streaming of input and output parameters limits task variability and automation.  
Developing a generalized parameter serialization and deserialization layer that supports arbitrary kernel arguments would increase flexibility and reduce overhead in task preparation.

Future work may also explore the use of persistent kernels for GPU partitioning and automatic task generation.
By fully allocating GPU resources, individual \acsp{SM} can be assigned dedicated task queues, keeping different modules independent from one another. 
This form of GPU partitioning reduces interference between tasks and improves isolation.
Furthermore, implementing a CUDA API interception layer to capture and schedule tasks automatically would significantly enhance both the use of persistent threads in other projects and the ease of adapting these tasks to new workloads.

\section{Conclusion}

Overall, while the current implementation demonstrates the feasibility of persistent GPU threads, it remains a relatively more straightforward exploration of the concept.
There is substantial opportunity for further optimization, refinement, and scaling to unlock the full potential of this approach and incorporate additional real-time scheduling strategies into the framework.
Future work could build on this foundation to develop more sophisticated scheduling strategies, enhance automation, and provide better support for complex real-time workloads.


%If too many tasks are scheduled or the memory buffer becomes fully saturated, task enqueueing fails and the launch function returns a \texttt{-1} error code.  
%Therefore, proper usage requires either a fault-tolerant launching mechanism that can handle task rejections or a carefully designed launch strategy to avoid buffer overflow.




