\chapter{Luisa Coroutines}\label{chapter:Coroutines}

The first proposed approach to implementing a GPU scheduler for autonomous driving focuses on integrating the LuisaCompute coroutine platform into Apollo.
The goal was to enable GPU coroutines within Apollo, allowing the autonomous driving framework to suspend and resume GPU kernel execution.
This capability would allow the scheduler to better enforce bounded response latencies by directly scheduling the highest priority tasks at the appropriate time, rather than waiting for current GPU workloads to complete.

\section{Coroutines}

Coroutines are a form of asynchronous programming that enable cooperative multitasking between functions.
Unlike thread- or process-level context switches, which involve greater overhead, coroutines maintain only a function-level context, allowing fast and lightweight task switching.
This makes them particularly useful for enabling runtime kernel task switching on the GPU. 
Here execution of a kernel can be suspended to allow another kernel to run, and then later resumed without blocking other work.

A coroutine suspends execution by capturing its current context, known as the continuation, which contains the execution state needed for later resumption \cite{Zheng2022LuisaRender}.
Once suspended, another task can execute without overwriting or disrupting the saved kernel state.
When the interim task completes, the coroutine can continue exactly where it left off by restoring its continuation.
This ability to strategically pause and resume execution makes coroutines well suited for real time GPU workloads, where rapid switching between concurrent tasks can help meet hard deadlines without delay.


\section{CPU Coroutines}

Unlike threads or processes, which require greater context switching, the use of coroutines allows for simple, faster context switching between different functions. 
On x86 architecture, the CPU calling convention for functions pushes the next instruction address to the stack and jumps to the instruction address of the function call.
The new functions accesses the variables through the stack and registers. 
In this convention, available registers are divided into volatile, caller saved registers and non volatile, callee saved registers. 
If manipulated, the callee saved registers must be restored before the function terminates, while the caller saved registers may be manipulated. 

After the function call executes the return instruction, the next instruction address is read off of the stack and executed.
For a function to pause execution, the minimum context is these callee saved registers, as these are the registers that must be reproduced. 
Further additional context can manually be pushed to the stack, which becomes restored when the next coroutine terminates. 


Consider the following CPU coroutine code taken from Apollo. 

\begin{lstlisting}[language=x86asm,caption={CPU Coroutine}, label={lst:coro}]
ctx_swap:
    pushq %rdi
    pushq %r12
    pushq %r13
    pushq %r14
    pushq %r15
    pushq %rbx
    pushq %rbp
    movq %rsp, (%rdi)

    movq (%rsi), %rsp
    popq %rbp
    popq %rbx
    popq %r15
    popq %r14
    popq %r13
    popq %r12
    popq %rdi
    ret
\end{lstlisting}


This function ctx\_swap, context swap, receives two parameters, \%rdi and \%rsp, both addresses to store and retrieve the coroutine continuations. 
The first section of ctx\_swap, until like 10, saves the current continuation, by pushing all the callee saved registers onto the current stack.
The stack address of the current continuation, \%rsp, is then saved in the register \%rdi, which will be saved by the function for switching the context back to the original context.
The second half of the context switching function loads the new context from the stack. 
First, the stack pointer is updated with the new coroutine's continuation stack pointer. 
Using this new stack location, all the callee saved registers are loaded in the reverse order to the context saved order and loaded into the registers for the new function. 
In comparison to the context switching used by either processes or threads, this executes very quickly.

\section{GPU Coroutines}

In contrast to CPU coroutines, the GPU coroutines are more complex due to the large number of concurrent threads and concurrent warps that are executing and the specifics of GPU programming, especially in regards to the stack management.
GPUs launch kernels that can not be interrupted or yield their resources through the kernel's lifetime.
The kernel function will saturate the number of warps and threads that were allocated to it until termination.
After each kernel terminates the state afterwards is not preserved for the next incoming kernel. 
Furthermore, unlike \acs{CPU} function calls where the instruction pointer is pushed onto the stack and the program then jumps to the new function, \acsp{GPU} save stack pressure by aggressively inlining function calls. 
The advantage of inlining function calls is that there is no overhead when beginning a new function as that function code is already readily available. 
Unfortunately for tasks dependent on deep call stacks such as recursion, this leads to an exponentially large instruction memory. 
The underlying PTX code allows for recursion using the \lstinline[language=cuda]`nvcc -rdc=true` flag.



Due to the management of call stacks and batch execution of kernels on \acsp{GPU}, the \acs{GPU} requires a persistent kernel implementation with a manual implementation of coroutine state in order to save and resume their continuation. 
Due to the nature of kernels executing until completion, the \acs{GPU} coroutine scheduler needs to be based on persistent threads, which schedule coroutines onto their threads. 
These coroutines themselves need to have suspension points to manually give control back to the scheduler in order to execute the next task. 
Saving every register value for every thread across every coroutine is to computationally expensive, so the coroutine contexts need to be managed locally and saved in global memory to free up limited \acs{SM} resources for new tasks. 

The implementation developed in LuisaCompute-Coroutine enables GPU coroutines by providing a coroutine based API that acts as a \acs{JIT} compiler for generating GPU kernels at runtime. 
LuisaCompute offers a DSL embedded in C++, allowing programmers to explicitly define coroutine suspension points using standard C++20 coroutine syntax, such as co\_await. 
These coroutine constructs are not executed immediately but are instead interpreted symbolically into an \acs{AST}.

At runtime, LuisaCompute builds a symbolic representation of the kernel's control and data flow in an abstract syntax tree (AST), through operator overloading and expression tracking. 
This symbolic trace is then lowered into an intermediate representation (IR), which encodes the coroutine as a state machine, capturing both the control flow and the coroutine's execution context. 
The resulting IR is compiled into GPU code, such as PTX for CUDA, using LuisaComputeâ€™s JIT backend. 
Once compiled, these coroutine-based kernels are dispatched and executed on persistent GPU threads, which maintain their state across kernel invocations and facilitate efficient asynchronous execution and task switching on the GPU.


As part of this work, I initially explored the possibility of integrating LuisaCompute coroutines into the Apollo autonomous driving platform. 
However, due to the lack of documentation and my limited understanding of both Apollo and LuisaCompute in both the implementation of tasks into Apollo and the underlying abstract syntax tree (AST) and intermediate representations (IRs) used in LuisaCompute's JIT compilation system, I struggled with dependency issues and was ultimately unable to complete the integration. 
Rather than continuing down this path, I decided to simplify the problem and shift focus toward developing a custom implementation of persistent GPU threads, which still reduce the overhead involved with launching \acs{GPU} kernels. 


