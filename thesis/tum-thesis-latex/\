% !TeX root = ../main.tex

\chapter{Persistent Threads}\label{chapter:Persistent Threads}

Persistent GPU threads allow the hardware resources to be partitioned and reduce kernel execution and scheduling overhead and are fundamental for other applications seeking to enhance GPU scheduling.  
Many other applications such as coroutines or mega kernels are based on persistent threads as a means of reducing kernel launch overhead and allow further control in application specific scheduling decisions, otherwise black boxed by the cuda api. 
Running persistent threads essentially allows the memory and system configurations to be loaded in ahead of runtime and reduces overhead during execution.

Failing to implement LuisaCompute's coroutines into Apollo, the goal shifted to finding a manual GPU scheduling functionality to implement and use. 
In restricting the scope from GPU coroutines to simply implementing persistent threads into Apollo, this thesis attempted to find an open source implementation which would allow the fine grained scheduling controll specific to Apollo. 
Unfortunately, given that most persistent thread implementations are highly specific they are not open source, which led to selecting an implementation that required more work to make it feasible. 
The only open source persistent thread implementation that was readily available was LightKer, a research project, which measured the hypothetical speedup of using persistent threads over sequential kernel launches. 
Seeking to implement LightKer into Apollo first required a complete restructuring of the code base to support a real time system.

The LightKer implementation itself intended to measure the performance difference between sequential kernel executions versus a persistent kernel implementation.
This implementation constructed simple trivial kernels and tested the overhead difference between calling them explicitely from the host in kernel launches versus implicitely in the persistent kernel. 
Unfortunately, this application does not support variable tasks or memory transfers at runtime, necessary to pass arguments and results back and forth. 
As such, the implementation only succeeds in measuring latency differences between scheduling tasks using a device side while loop versus explicit kernel launches. 
However, the LightKer implementation does provide a simple framework for using a GPU-Host mailbox for scheduling kernel tasks as well as a helpful persistent kernel launch structure. 

\section{Architecture}

From a high level, the architecture implemented into this project appears as follows, with the GPU-Host Mailbox, being used from the Lightkernel project as well as the internal structure.

\begin{figure}[H]
  \centering
  \resizebox{1.0\linewidth}{!}{
	  \input{figures/architecture.tex}
  }
  \caption{Persistent Thread Architecture}
  \label{fig:architecture}
\end{figure}


At a high level, there are three important components: task queue, memory buffer, and the persistent threads themselves.
The task queue is managed, controlled, and allocated by the host and provides the arguments and tasks for the GPU to execute. 
The implementation of the task queue gives the programmer fine-tuned implementation oppertunities to manually design and schedule workloads. 
The memory buffers provides an epoch based staging area for input arguments and output results as well as persistent memory for the further extension of coroutines.
Lastly, the persistent threads are the fundamental execution units executing the \acs{GPU} code.  

The Figure~\ref{fig:architecture} depicts these individual components in a logical program architecture overview.
The left side of the graphic shows the host side code and methods which allow the enqueuing of tasks and launching of the persistent thread kernel.  
On the right hand side of the graphic, the actual device side code allocation and kernel execution is depicted which allows the processing of memory and write back of results to the memory buffers.
The mailboxes are constantly enqueuing and dequeuing new tasks throughout the execution of the kernels.

\section{Implementation}

GPU kernels are essentially GPU functions, which are scheduled from the host process, that take parameters and a logical ordering of threads to be partitioned and across the \acsp{SM}.
In order to replicate both of these restrictions without explicitely launching kernels, the persistent thread needs to be able to store the functions and assign them to waiting compute resources.
Similar to other persistent thread implementations, this project implemented a task queue, which stores the function context needed to execute the GPU code. 

\subsection{Task Queue}

The task queue functions as a cache between the host and the executing code allowing the preemptive scheduling of tasks.
Instead of allowing the cuda driver to partion the GPU and assign the tasks to threads, the task parameters are stored in allocated memory until the GPU is free to execute the next task.
This memory remains allocated throughout the duration of the persistent thread kernel. 
The context for the GPU kernel depends on the explicit function and the parameters for that function.
This implementation can be furthered to include additional memory used for storing the continuation of a function for the implementation of GPU coroutines. 


\subsection{Function Pointers}

To execute new functions from the persistent threads, the task queue needs to be able to reference the specific function. 
Generally referencing functions on a \acs{CPU} requires only the function pointer to execute the code defined at that memory location. 
When GPU functions are compiled, the device code lives in the GPU address space and is not accessible from the CPU.  
The CPU only has access to functions denoted by the \lstinline[language=cuda]`__global__` keyword, which allows the execution of GPU kernels, not enqueuing of GPU functions.
In order to be able to access and run the functions specified by the CPU, the task queue supports a lookup table to map integers to specific functions.  
The lookup table allows the host to memcpy in function ids to the task queue when enqueuing new tasks.


\subsection{Function Parameters}

When the CPU assigns tasks to the GPU, it passes either allocated GPU memory pointers or explicit parameters. 
These explicit parameters then get propogated to all the individual threads executing the kernel code, resulting in greater api memory overhead. 
When enqueuing new tasks to the task queue, the memory has to be transfered at runtime before the device function calls. 

The GPU task in the queue originally had a pointer to the allocated memory and upon recieving compute resources would schedule the task with the memory to the individual persistent thread.
Unfortunately, this method is dependent on the specific task and parameters and consumes variable memory requiring further pointers to GPU memory.  
In order to consolidate the memory pointers, the task queue was simplified to contain only allocated memory pointers in order to automatically load kernel memory. 


In this method, enqueing the GPU tasks forces the programmer to streamify the data and automatically load the memory into preallocated memory partitions. 
The task queue then only consists of the actual memory partition pointers, both start and end. 
Executing a task then requires the interpretation of the memory and then the loading of it into the device function.
Manually extending this method allows the user to manually allocate more memory than is needed and use that memory to yield and run coroutines.


\subsection{Memory Model}
In order to provide the incoming scheduled kernels a staging area for allocated memory, the implementation contains a running epoch memory model. 


\subsection{}
